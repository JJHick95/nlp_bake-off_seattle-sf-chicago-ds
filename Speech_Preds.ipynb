{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import scipy\n",
    "import helpers\n",
    "import types\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('X_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_pickle('y_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35     Chief Justice Roberts, President Carter, Pres...\n",
       "13     Fellow-Citizens:     We have assembled to rep...\n",
       "26     My friends, before I begin the expression of ...\n",
       "30     Senator Hatfield, Mr. Chief Justice, Mr. Pres...\n",
       "16     My Fellow-Citizens:     When we assembled her...\n",
       "31     Mr. Chief Justice, Mr. President, Vice Presid...\n",
       "21     My Countrymen:     This occasion is not alone...\n",
       "12     Fellow-Citizens:     Under Providence I have ...\n",
       "8      Called from a retirement which I had supposed...\n",
       "17     My fellow-citizens, no people on earth have m...\n",
       "9      Fellow-Citizens:    Without solicitation on m...\n",
       "34     My fellow citizens:  I stand here today humbl...\n",
       "0      Fellow Citizens:   I am again called upon by ...\n",
       "4      I should be destitute of feeling if I was not...\n",
       "29     For myself and for our Nation, I want to than...\n",
       "15     Fellow-Citizens:     In the presence of this ...\n",
       "19      There has been a change of government. It be...\n",
       "5      In compliance with an usage coeval with the e...\n",
       "11     In compliance with a custom as old as the Gov...\n",
       "1      When it was first perceived, in early times, ...\n",
       "24     Mr. Chief Justice, Mr. Vice President, my fri...\n",
       "2       Called upon to undertake the duties of the f...\n",
       "33     Vice President Cheney, Mr. Chief Justice, Pre...\n",
       "3       March 4, 1813   About to add the solemnity o...\n",
       "32     My fellow citizens :   Today we celebrate the...\n",
       "23     When four years ago we met to inaugurate a Pr...\n",
       "27     My fellow countrymen, on this occasion, the o...\n",
       "10     Fellow-Citizens:  I appear before you this da...\n",
       "22     I am certain that my fellow Americans expect ...\n",
       "18     My Fellow-Citizens:     Anyone who has taken ...\n",
       "25     Mr. Vice President, Mr. Chief Justice, and fe...\n",
       "6      Fellow-Citizens:   The will of the American p...\n",
       "20     My Countrymen:  No one can contemplate curren...\n",
       "7      Fellow-Citizens:  The practice of all my pred...\n",
       "14     Fellow-Citizens:  We stand to-day upon an emi...\n",
       "28     Senator Dirksen, Mr. Chief Justice, Mr. Vice ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35     R\n",
       "13     R\n",
       "26     R\n",
       "30     R\n",
       "16     R\n",
       "31     R\n",
       "21     R\n",
       "12     R\n",
       "8      W\n",
       "17     R\n",
       "9      D\n",
       "34     D\n",
       "0      F\n",
       "4     DR\n",
       "29     D\n",
       "15     D\n",
       "19     D\n",
       "5     DR\n",
       "11     R\n",
       "1      F\n",
       "24     D\n",
       "2     DR\n",
       "33     R\n",
       "3     DR\n",
       "32     D\n",
       "23     D\n",
       "27     D\n",
       "10     D\n",
       "22     D\n",
       "18     R\n",
       "25     D\n",
       "6      D\n",
       "20     R\n",
       "7      D\n",
       "14     R\n",
       "28     R\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 0, 0, 2, 1, 0, 0, 0, 1, 3, 2, 0, 1,\n",
       "       3, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of stopwords\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "# Generate tf-idf vectorization (use sklearn's TfidfVectorizer) for our data\n",
    "def tfidf(X, y,  stopwords_list, random_state=42):\n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify\n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    stopwords_list: list ojbect\n",
    "        List containing words and punctuation to remove.\n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vecotrizer object\n",
    "    '''\n",
    "    # Generate a train test split of X and y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n",
    "    #This tool removes case punctuation, numbers, and stopwords\n",
    "    tfidf = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:[a-z]+)?)\", stop_words=stopwords_list)\n",
    "    #Fitting and transforming on training set\n",
    "    tf_idf_train = tfidf.fit_transform(X_train)\n",
    "    #transforming on testing set\n",
    "    tf_idf_test = tfidf.transform(X_test)\n",
    "    #Return\n",
    "    return tf_idf_train, tf_idf_test, y_train, y_test, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vectorizer = tfidf(X, y, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a classifier, trains it on our tf-idf vectors,\n",
    "# and generates train and test predictiions\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    # a) fit the classifier with our training data\n",
    "    classifier.fit(tf_idf_train, y_train)\n",
    "    # b) predict the labels of our train data and store them in train_preds\n",
    "    train_preds = classifier.predict(tf_idf_train)\n",
    "    # c) predict the labels of our test data and store them in test_preds\n",
    "    test_preds = classifier.predict(tf_idf_test)\n",
    "    # d) return train_preds and test_preds\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_preds, nb_test_preds = classify_text(nb, tf_idf_train,\n",
    "                                              tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helpers' has no attribute 'metrics_printout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-72e28d3f7cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_printout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_test_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_test_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'helpers' has no attribute 'metrics_printout'"
     ]
    }
   ],
   "source": [
    "helpers.metrics_printout(y_test, nb_test_preds, rf_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5925925925925926"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, nb_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, nb_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc =RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_preds, rfc_test_preds = classify_text(rfc, tf_idf_train,\n",
    "                                              tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, rfc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, rfc_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.1111111111111111\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.1111111111111111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonhickey/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tf_idf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tf_idf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111\n",
      "[[0 0 0 4 0]\n",
      " [0 0 0 2 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tf_idf_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tf_idf_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.01582304685787793\n",
      "  (0, 17)\t0.026451448533411803\n",
      "  (0, 24)\t0.015111275922459927\n",
      "  (0, 37)\t0.022272790010323062\n",
      "  (0, 52)\t0.01582304685787793\n",
      "  (0, 60)\t0.048196300184983706\n",
      "  (0, 71)\t0.052902897066823606\n",
      "  (0, 76)\t0.029768236397967787\n",
      "  (0, 81)\t0.044545580020646125\n",
      "  (0, 90)\t0.03827393244461992\n",
      "  (0, 113)\t0.026451448533411803\n",
      "  (0, 128)\t0.020781362227935866\n",
      "  (0, 131)\t0.036786166869385226\n",
      "  (0, 133)\t0.024098150092491853\n",
      "  (0, 136)\t0.024098150092491853\n",
      "  (0, 138)\t0.048196300184983706\n",
      "  (0, 139)\t0.017464574363379875\n",
      "  (0, 141)\t0.026451448533411803\n",
      "  (0, 145)\t0.01660270370484712\n",
      "  (0, 146)\t0.01582304685787793\n",
      "  (0, 150)\t0.024098150092491853\n",
      "  (0, 157)\t0.026451448533411803\n",
      "  (0, 165)\t0.015111275922459927\n",
      "  (0, 180)\t0.029768236397967787\n",
      "  (0, 184)\t0.029768236397967787\n",
      "  :\t:\n",
      "  (26, 6649)\t0.025130424248545478\n",
      "  (26, 6652)\t0.033952053710905\n",
      "  (26, 6661)\t0.02241685048267132\n",
      "  (26, 6678)\t0.02606389298978702\n",
      "  (26, 6679)\t0.02858848965367639\n",
      "  (26, 6686)\t0.019396247850532695\n",
      "  (26, 6703)\t0.012118344920346543\n",
      "  (26, 6734)\t0.015138947552485174\n",
      "  (26, 6736)\t0.026674150780718844\n",
      "  (26, 6752)\t0.047307096297160435\n",
      "  (26, 6764)\t0.018555815871502398\n",
      "  (26, 6767)\t0.023653548148580218\n",
      "  (26, 6770)\t0.030931451078766367\n",
      "  (26, 6779)\t0.03820935400895251\n",
      "  (26, 6782)\t0.015138947552485174\n",
      "  (26, 6783)\t0.04209805137991225\n",
      "  (26, 6793)\t0.030931451078766367\n",
      "  (26, 6800)\t0.03769563637281821\n",
      "  (26, 6808)\t0.0629987667927049\n",
      "  (26, 6815)\t0.033952053710905\n",
      "  (26, 6818)\t0.03909583948468053\n",
      "  (26, 6829)\t0.033952053710905\n",
      "  (26, 6837)\t0.012565212124272739\n",
      "  (26, 6841)\t0.015138947552485174\n",
      "  (26, 6846)\t0.023653548148580218\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000people', '100', '15th', '16', '1774', '1776', '1778', '1780', '1787']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "# Transform the training data: tfidf_train \n",
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)\n",
    "# Transform the test data: tfidf_test \n",
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)\n",
    "# Print the first 10 features\n",
    "print(tf_idf_vectorizer.get_feature_names()[:10])\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tf_idf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[[1 0 1 0]\n",
      " [2 0 0 0]\n",
      " [0 0 3 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tf_idf_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tf_idf_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.375\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.375\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.5\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonhickey/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tf_idf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tf_idf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [1 0 2 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "rfc_classifier = RandomForestClassifier()\n",
    "# Fit the classifier to the training data\n",
    "rfc_classifier.fit(tf_idf_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = rfc_classifier.predict(tf_idf_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonhickey/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [2, 5, 10],\n",
       "                         'n_estimators': [100, 1000, 2000]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# # random forest\n",
    "g4 = {\n",
    "    'max_depth': [ 2, 5, 10 ],\n",
    "    'n_estimators': [ 100, 1000, 2000]\n",
    "}\n",
    "gs4 = GridSearchCV(RandomForestClassifier(), g4)\n",
    "gs4.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, n_estimators=1000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rfc = gs4.best_estimator_\n",
    "best_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [2 0 1 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create the predicted tags: pred\n",
    "pred = best_rfc.predict(tf_idf_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [1 0 2 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "rfc_classifier = RandomForestClassifier(n_estimators=1000, max_depth=2)\n",
    "# Fit the classifier to the training data\n",
    "rfc_classifier.fit(tf_idf_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = rfc_classifier.predict(tf_idf_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000people', '100', '15th', '16', '1774', '1776', '1778', '1780', '1787']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[[1 0 1 0]\n",
      " [2 0 0 0]\n",
      " [0 0 3 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [2 0 1 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "rfc_classifier = RandomForestClassifier(n_estimators=1000)\n",
    "# Fit the classifier to the training data\n",
    "rfc_classifier.fit(count_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = rfc_classifier.predict(count_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jonhickey/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonhickey/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'abroad', 'advance', 'affair', 'age', 'ago', 'almighty', 'ancient', 'ask', 'assume']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                strip_accents = 'unicode', # works \n",
    "                                stop_words = 'english', # works\n",
    "                                lowercase = True, # works\n",
    "                                max_df = 0.5, # works\n",
    "                                min_df = 10)\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "[[2 0 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D' 'R' 'R' 'D' 'R' 'R' 'R' 'D']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing  import LabelEncoder\n",
    "\n",
    "decoded_array = le.inverse_transform(pred)\n",
    "print(decoded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Fellow-Citizens of the Senate and of the Hou...\n",
       "13      My Countrymen:     When one surveys the worl...\n",
       "8      Citizens of the United States:     Your suffr...\n",
       "1       Proceeding, fellow citizens, to that qualifi...\n",
       "15     THE PRICE OF PEACE     Mr. Chairman, Mr. Vice...\n",
       "5      Elected by the American people to the highest...\n",
       "20     President Clinton, distinguished guests and m...\n",
       "11     Fellow-Citizens:     In obedience to the will...\n",
       "3      I shall not attempt to describe the grateful ...\n",
       "4       Fellow-Citizens:   About to undertake the ar...\n",
       "17     Mr. Vice President, Mr. Speaker, Mr. Chief Ju...\n",
       "12      My Fellow Citizens:     The four years which...\n",
       "18     Senator Mathias, Chief Justice Burger, Vice P...\n",
       "16     Vice President Johnson, Mr. Speaker, Mr. Chie...\n",
       "2       March 4, 1809   Unwilling to depart from exa...\n",
       "9      Fellow-Citizens:     There is no constitution...\n",
       "21     Vice President Biden, Mr. Chief Justice,  mem...\n",
       "7       At this second appearing to take the  oath  ...\n",
       "10     My Fellow-Citizens:     In obedience of the m...\n",
       "14     On each national day of inauguration since 17...\n",
       "19     My fellow citizens :   At this last president...\n",
       "6      My Countrymen:   It a relief to feel that no ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using only the 'text' column values: count_test \n",
    "final_test = count_vectorizer.transform(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb_classifier.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DR' 'R' 'R' 'DR' 'D' 'F' 'D' 'R' 'F' 'F' 'R' 'R' 'D' 'R' 'F' 'F' 'D' 'D'\n",
      " 'D' 'R' 'R' 'R']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing  import LabelEncoder\n",
    "\n",
    "decoded_array = le.inverse_transform(preds)\n",
    "print(decoded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sr_jh_predictions.pkl', 'wb') as f:\n",
    "    pkl.dump(decoded_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(decoded_array).astype(str)\n",
    "df[0] = df[0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('sr_jh_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "read",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-c198fbfb792b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfileObject2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodelInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileObject2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mfileObject2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: read"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "arrayInput = decoded_array #Trial input\n",
    "save = True\n",
    "load = True\n",
    "\n",
    "fileName = 'sr_jh_predictions.pkl'\n",
    "fileObject = open(fileName, 'wb')\n",
    "\n",
    "if save:\n",
    "    pkl.dump(arrayInput, fileObject)\n",
    "    fileObject.close()\n",
    "\n",
    "if load:\n",
    "    fileObject2 = open(fileName, 'wb')\n",
    "    modelInput = pkl.load(fileObject2)\n",
    "    fileObject2.close()\n",
    "\n",
    "if arrayInput == modelInput:\n",
    "    Print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(decoded_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
